# ========== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ì–ò–ë–†–ò–î–ù–ê–Ø RAG –°–ò–°–¢–ï–ú–ê ==========
from typing import Dict, List

from pathlib import Path

# LangChain - –∏—Å–ø–æ–ª—å–∑—É–µ–º langchain_core –Ω–∞–ø—Ä—è–º—É—é
from langchain_core.documents import Document
from langchain_experimental.text_splitter import SemanticChunker
from langchain_text_splitters import RecursiveCharacterTextSplitter

from openai import OpenAI

from services.models import SearchResult
from services.entity_extractor import EntityExtractor
from services.neo4j_manager import Neo4jGraphManager
from services.qdrant_manager import QdrantVectorManager

from config import *


class HybridRAGSystem:
    """
    –ì–∏–±—Ä–∏–¥–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ —Å —É–º–Ω—ã–º chunking
    """

    def __init__(self, embeddings, qdrant_path, collection_name,
                 neo4j_uri, neo4j_user, neo4j_password, llm_api_key: str, llm_base_url: str):
        self.embeddings = embeddings
        self.qdrant = QdrantVectorManager(QDRANT_HOST, QDRANT_PORT, collection_name, VECTOR_SIZE)
        self.neo4j = Neo4jGraphManager(neo4j_uri, neo4j_user, neo4j_password)
        self.entity_extractor = EntityExtractor()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞ Cloud.ru (GigaChat)
        self.llm_client = OpenAI(
            api_key=llm_api_key,
            base_url=f"{llm_base_url}"
        )

        self.llm_model = "GigaChat/GigaChat-2-Max"

        # –î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è chunking

        # 1. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π chunker (–¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
        self.pre_splitter = RecursiveCharacterTextSplitter(
            chunk_size=5000,  # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
            chunk_overlap=500,
            separators=["\n\n\n", "\n\n", "\n", ". ", " ", ""]
        )

        # 2. Semantic chunker (–¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)
        try:
            self.semantic_splitter = SemanticChunker(
                embeddings=embeddings,
                breakpoint_threshold_type="percentile",
                breakpoint_threshold_amount=0.6
            )
            self.use_semantic = True
            print("‚úì Semantic chunking –≤–∫–ª—é—á–µ–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è  Semantic chunking –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è RecursiveCharacterTextSplitter")
            self.use_semantic = False

        # 3. Fallback chunker (–µ—Å–ª–∏ semantic –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)
        self.fallback_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )

        print("‚úì –ì–∏–±—Ä–∏–¥–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    def _smart_chunk_text(self, text: str, metadata: Dict) -> List[Document]:
        """
        –£–º–Ω—ã–π chunking —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback

        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
        1. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç < 5000 —Å–∏–º–≤–æ–ª–æ–≤ ‚Üí semantic chunking –Ω–∞–ø—Ä—è–º—É—é
        2. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç > 5000 —Å–∏–º–≤–æ–ª–æ–≤ ‚Üí —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Ä–µ–∑–∫–∞,
           –ø–æ—Ç–æ–º semantic –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫—É—Å–∫–∞
        3. –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Üí fallback –Ω–∞ RecursiveCharacterTextSplitter
        """
        text_length = len(text)
        source = metadata.get('source', 'unknown')

        print(f"  üìè –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {text_length:,} —Å–∏–º–≤–æ–ª–æ–≤")

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ú–∞–ª—ã–π —Ç–µ–∫—Å—Ç ‚Äî semantic –Ω–∞–ø—Ä—è–º—É—é
        if text_length < 5000 and self.use_semantic:
            try:
                print(f"  üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—è: Semantic chunking (–º–∞–ª—ã–π —Ç–µ–∫—Å—Ç)")
                chunks = self.semantic_splitter.create_documents(
                    texts=[text],
                    metadatas=[metadata]
                )
                print(f"  ‚úì –°–æ–∑–¥–∞–Ω–æ {len(chunks)} semantic chunks")
                return chunks
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Semantic chunking failed: {str(e)[:100]}")
                print(f"  üîÑ Fallback –Ω–∞ RecursiveCharacterTextSplitter")

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ë–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç ‚Äî –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        if text_length >= 5000:
            print(f"  üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π chunking (–±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç)")

            # –≠—Ç–∞–ø 1: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Ä–µ–∑–∫–∞ –Ω–∞ –±–æ–ª—å—à–∏–µ –∫—É—Å–∫–∏
            pre_chunks = self.pre_splitter.create_documents(
                texts=[text],
                metadatas=[metadata]
            )
            print(f"  üìä –≠—Ç–∞–ø 1: {len(pre_chunks)} –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")

            # –≠—Ç–∞–ø 2: Semantic chunking –∫–∞–∂–¥–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –∫—É—Å–∫–∞
            all_final_chunks = []

            for i, pre_chunk in enumerate(pre_chunks):
                if self.use_semantic:
                    try:
                        # –ü–æ–ø—ã—Ç–∫–∞ semantic chunking
                        semantic_chunks = self.semantic_splitter.create_documents(
                            texts=[pre_chunk.page_content],
                            metadatas=[pre_chunk.metadata]
                        )
                        all_final_chunks.extend(semantic_chunks)
                    except Exception as e:
                        # Fallback –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞–Ω–∫–∞
                        print(f"    ‚ö†Ô∏è  Semantic failed –¥–ª—è —á–∞–Ω–∫–∞ {i + 1}, –∏—Å–ø–æ–ª—å–∑—É—é fallback")
                        fallback_chunks = self.fallback_splitter.create_documents(
                            texts=[pre_chunk.page_content],
                            metadatas=[pre_chunk.metadata]
                        )
                        all_final_chunks.extend(fallback_chunks)
                else:
                    # Semantic –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback —Å—Ä–∞–∑—É
                    fallback_chunks = self.fallback_splitter.create_documents(
                        texts=[pre_chunk.page_content],
                        metadatas=[pre_chunk.metadata]
                    )
                    all_final_chunks.extend(fallback_chunks)

            print(f"  ‚úì –≠—Ç–∞–ø 2: {len(all_final_chunks)} —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            return all_final_chunks

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: Fallback (–µ—Å–ª–∏ –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ)
        print(f"  üîÑ Fallback –Ω–∞ RecursiveCharacterTextSplitter")
        chunks = self.fallback_splitter.create_documents(
            texts=[text],
            metadatas=[metadata]
        )
        print(f"  ‚úì –°–æ–∑–¥–∞–Ω–æ {len(chunks)} fallback chunks")
        return chunks

    def create_knowledge_base(self, processed_files: List[Dict]):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        print(f"\n{'=' * 60}")
        print(f"üî® –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ {len(processed_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        print("=" * 60)

        all_chunks = []

        for idx, file_info in enumerate(processed_files, 1):
            print(f"\n[{idx}/{len(processed_files)}] üìÑ {file_info['original_file']}")

            try:
                # –£–º–Ω—ã–π chunking —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback
                chunks = self._smart_chunk_text(
                    text=file_info['text'],
                    metadata={
                        'source': file_info['original_file'],
                        'text_file': file_info.get('text_file', '')
                    }
                )

                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫
                for i, chunk in enumerate(chunks):
                    chunk_id = f"{Path(file_info['original_file']).stem}_chunk{i}"
                    chunk.metadata['chunk_id'] = chunk_id
                    chunk.metadata['chunk_index'] = i
                    chunk.metadata['total_chunks'] = len(chunks)

                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞)
                    text_for_entities = chunk.page_content[:10000]  # –õ–∏–º–∏—Ç –¥–ª—è spaCy
                    entities = self.entity_extractor.extract_entities(text_for_entities)

                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ Neo4j
                    self.neo4j.add_chunk_with_entities(
                        chunk_id=chunk_id,
                        content=chunk.page_content,
                        metadata=chunk.metadata,
                        entities=entities
                    )

                    all_chunks.append(chunk)

                print(f"  üï∏Ô∏è  –î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ –≥—Ä–∞—Ñ")

            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")
                import traceback
                traceback.print_exc()
                continue

        if not all_chunks:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞!")

        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–∞–º–∏
        print(f"\nüîç –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(all_chunks)} —á–∞–Ω–∫–æ–≤...")
        chunk_texts = [c.page_content for c in all_chunks]

        batch_size = 32  # –ë–∞—Ç—á–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        all_embeddings = []

        for i in range(0, len(chunk_texts), batch_size):
            batch = chunk_texts[i:i + batch_size]
            try:
                batch_embeddings = self.embeddings.embed_documents(batch)
                all_embeddings.extend(batch_embeddings)
                print(f"  ‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {min(i + batch_size, len(chunk_texts))}/{len(chunk_texts)}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {i // batch_size + 1}: {e}")
                # –ü—ã—Ç–∞–µ–º—Å—è –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ –æ–¥–Ω–æ–º—É
                for text in batch:
                    try:
                        emb = self.embeddings.embed_query(text)
                        all_embeddings.append(emb)
                    except:
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –∫–∞–∫ fallback
                        all_embeddings.append([0.0] * VECTOR_SIZE)

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ Qdrant
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant...")
        self.qdrant.add_chunks(all_chunks, all_embeddings)

        print(f"\n{'=' * 60}")
        print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"   üìö –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
        print(f"   üìÅ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(processed_files)}")
        print(f"   üîç –í–µ–∫—Ç–æ—Ä–æ–≤ –≤ Qdrant: {len(all_embeddings)}")
        print("=" * 60)

    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[SearchResult]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        print(f"\nüîç –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: '{query}'")
        print(f"   Alpha (–≤–µ–∫—Ç–æ—Ä/–≥—Ä–∞—Ñ): {alpha:.2f}/{1 - alpha:.2f}")

        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        print(f"  üîç –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫...")
        query_vector = self.embeddings.embed_query(query)
        vector_results = self.qdrant.search(query_vector, top_k=top_k)
        print(f"     –ù–∞–π–¥–µ–Ω–æ: {len(vector_results)}")

        # –ì—Ä–∞—Ñ–æ–≤—ã–π –ø–æ–∏—Å–∫
        graph_results = self.neo4j.search_by_entities(query, top_k=top_k)
        print(f"     –ù–∞–π–¥–µ–Ω–æ: {len(graph_results)}")

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        all_results = {}

        if vector_results:
            max_score = max(r.score for r in vector_results)
            for r in vector_results:
                norm = r.score / max_score if max_score > 0 else 0
                if r.chunk_id not in all_results:
                    all_results[r.chunk_id] = r
                    all_results[r.chunk_id].score = norm * alpha

        if graph_results:
            max_score = max(r.score for r in graph_results)
            for r in graph_results:
                norm = r.score / max_score if max_score > 0 else 0
                if r.chunk_id not in all_results:
                    all_results[r.chunk_id] = r
                    all_results[r.chunk_id].score = norm * (1 - alpha)
                else:
                    all_results[r.chunk_id].score += norm * (1 - alpha)
                    all_results[r.chunk_id].source = 'hybrid'

        sorted_results = sorted(all_results.values(), key=lambda x: x.score, reverse=True)[:top_k]
        print(f"  ‚úÖ –ò—Ç–æ–≥–æ: {len(sorted_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n")

        return sorted_results

    # --- –ù–û–í–´–ô –ú–ï–¢–û–î: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ RAG ---
    def generate_answer(self, query: str, context: str) -> str:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ RAG

        prompt = f"""
                –¢—ã ‚Äî —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ.

                === –ö–û–ù–¢–ï–ö–°–¢ ===
                {context}

                === –í–û–ü–†–û–° ===
                {query}

                –û—Ç–≤–µ—Ç—å —Ç–æ—á–Ω–æ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É, –±–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤:
                """

        response = self.llm_client.chat.completions.create(
            model=self.llm_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )

        return response.choices[0].message.content

    def rag(self, query: str, top_k=5):
        # –ü–æ–ª–Ω—ã–π RAG-–ø—Ä–æ—Ü–µ—Å—Å: –ø–æ–∏—Å–∫ + LLM –æ—Ç–≤–µ—Ç

        search_results = self.hybrid_search(query, top_k)

        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = "\n\n".join([
            f"[{r.source.upper()} score={r.score:.3f}] {r.content}"
            for r in search_results
        ])

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç LLM
        answer = self.generate_answer(query, context)
        return answer, search_results

    def close(self):
        self.neo4j.close()

